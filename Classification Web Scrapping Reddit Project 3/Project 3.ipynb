{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Classification of 'jobs' and 'forhire' subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build a binary classification model to predict if a post on reddit belongs to the \"jobs\" or \"forhire\" subreddit. The model will be considered successful if both the F1 score and sensitivity are above 90%.\n",
    "\n",
    "Additionally, the project aims to provide insights on the key words and phrases that people use when discussing jobs on reddit, with the stakeholders being data science peers, job seekers and job posters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit is a popular social news, content, and discussions website where posts are organised according to subject into user-created 'subreddits'. Members submit content (such as images, texts, and links) to subreddits, which can then be voted on and commented by other members, creating an internet community of sorts around specific themes. In this project, I examined posts from two subreddits - [**r/jobs**](https://www.reddit.com/r/jobs/) and [**r/forhire**](https://www.reddit.com/r/forhire/).\n",
    "\n",
    "<img src='datasets/rjobsCapture.jpg' width = 700 align = center>\n",
    "<center><font size=2 color='grey'>(Fig 1. The frontpage of r/jobs as of 9pm, 22 Sep 2021.)</font></center>\n",
    "\n",
    "\n",
    "<img src='datasets/rforhireCapture.jpg' width = 700 align = center>\n",
    "<center><font size=2 color='grey'>(Fig 2. The frontpage of r/forhire as of 9pm, 22 Sep 2021.)</font></center>\n",
    "\n",
    "As both subreddits are related to employment, there is potential for misclassification with both groups created around a year apart. \n",
    "\n",
    "The differences in r/jobs vs r/forhire include the size of the communities (r/jobs is more than twice the size of r/forhire at 577k vs 246k) with the nature of the posts on r/jobs appear mostly related to queries for career advice. In contrast, most posts on r/forhire are related to jobs posting by either job seekers or job providers.\n",
    "\n",
    "The objective of this project is to create the best classification model to identify Reddit posts, with the goals as stated in the problem statement. For this project, the data has to be gathered manually using the Pushshift API.\n",
    "\n",
    "For the training data set, a total of 1,000 posts were each gathered from the r/jobs subreddit and r/forhire subreddit. As usable language data can both be found in the post title and post content, they were merged to create new variable. For the purpose of this project, models comprising of combinations of the following are considered:\n",
    "\n",
    "    - Pre-processing: 1) Tokenizer        2) Lemmatization\n",
    "    - Transformer:    1) Bigrams          2) Sentiment Analysis\n",
    "    - Model:          1) Random Forest    2) Voting classifier\n",
    "\n",
    "After assessing both models, the final model is a voting classifier that uses a random forest classifier, multinomial naive Bayes, and support vector classifier to predict the subreddit based on the text in a given post. This model has an F1 score of 96% and a recall score of 97%, and would therefore be considered successful. \n",
    "\n",
    "Recommendations to further improve the model include:\n",
    "    \n",
    "- Expand our model to other subreddits that concern employment such as r/careeradvice to increase the corpus of words\n",
    "- Get more data from other employment resources (e.g. LinkedIn, JobStreet etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score, plot_roc_curve, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pull n posts from a specific subreddit\n",
    "def get_reddit(subreddit, n_samples, verbose = False): \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "    'subreddit': subreddit,\n",
    "    'size':(n_samples if n_samples <= 100 else 100)}\n",
    "    \n",
    "    res = requests.get(url,params)\n",
    "    data = res.json()\n",
    "    df_final = pd.DataFrame(data['data'])\n",
    "    \n",
    "    count = n_samples if n_samples <= 100 else 100\n",
    "    n = n_samples - 100\n",
    "    \n",
    "    while n > 0:\n",
    "        if (verbose == True) & (count%100 == 0):\n",
    "            print(f'{count} rows generated')\n",
    "                \n",
    "        params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size':(n if n <= 100 else 100),\n",
    "        'before':data['data'][99]['created_utc'] - 700000 \n",
    "        }\n",
    "        res = requests.get(url, params)\n",
    "        data = res.json()\n",
    "        df = pd.DataFrame(data['data'])\n",
    "        df_final = df_final.append(df)\n",
    "        n -= 100\n",
    "        count += 100\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for 1000 extracts from jobs sub\n",
    "df_jobs = get_reddit('jobs', 1000)\n",
    "\n",
    "# create dataframe for 5000 extracts from forhire sub\n",
    "df_forhire = get_reddit('forhire', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of jobs extract\n",
    "df_jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check extract dates\n",
    "start = int(df_jobs['created_utc'][999])\n",
    "end = int(df_jobs['created_utc'][0])\n",
    "print(f'start date:{datetime.fromtimestamp(start)}')\n",
    "print(f'end date:{datetime.fromtimestamp(end)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first 5 rows for job extract\n",
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of forhire extract\n",
    "df_forhire.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check extract dates\n",
    "start = int(df_forhire['created_utc'][999])\n",
    "end = int(df_forhire['created_utc'][0])\n",
    "print(f'start date:{datetime.fromtimestamp(start)}')\n",
    "print(f'end date:{datetime.fromtimestamp(end)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first 5 rows for forhire extract\n",
    "df_forhire.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "\n",
    "df_jobs.to_csv('datasets/jobs.csv', index= False)\n",
    "df_forhire.to_csv('datasets/forhire.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import raw files\n",
    "\n",
    "df_jobs = pd.read_csv('datasets/jobs.csv')\n",
    "df_forhire = pd.read_csv('datasets/forhire.csv')\n",
    "\n",
    "# drop all columns except 'title', 'selftext' and 'subreddit'\n",
    "\n",
    "col = ['title', 'selftext','subreddit', 'upvote_ratio']\n",
    "df_jobs = df_jobs[col]\n",
    "df_forhire = df_forhire[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 jobs subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check first 5 rows\n",
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "df_jobs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Out of 1000 rows, 66 are null values for selftext (i.e. the post has a title but no text) and we shall investigate these rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[df_jobs['selftext'].isnull()].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['selftext'] = df_jobs['selftext'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values again\n",
    "df_jobs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[df_jobs['selftext'] != ''].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[df_jobs['selftext']=='[removed]'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[df_jobs['selftext']=='[deleted]'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Out of 1000 posts, 110 were removed by the moderators, and 2 were deleted by the users themselves\n",
    "- I will delete these posts for the purpose of this project because the goal is to identify posts that are similar to what users post in the jobs subreddit, so I do not want to train my model on posts that moderators have already identified as irrelevant to this subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove \"removed\" and \"deleted\" posts\n",
    "def remove_del(df):\n",
    "    mask = np.logical_not(df['selftext'].isin(['[removed]','[deleted]']))\n",
    "    return df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = remove_del(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove filler text posts\n",
    "df_jobs = df_jobs[~df_jobs['selftext'].str.contains('filler')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check shape\n",
    "df_jobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For analysis, I want to combine the title and self text columns\n",
    "- My classifications will be based on the words in both the title and text of the reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['text'] = df_jobs['title'] + df_jobs['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df_jobs[df_jobs['text'].duplicated()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows\n",
    "df_jobs.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 forhire subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check first 5 rows\n",
    "df_forhire.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "df_forhire.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Out of 1000 rows, 15 are null values for selftext (i.e. the post has a title but no text) and we shall investigate these rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forhire[df_forhire['selftext'].isnull()].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I will replace the selftext with blank (same as I did for jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forhire['selftext'] = df_forhire['selftext'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forhire[df_forhire['selftext'] != ''].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forhire[df_forhire['selftext']=='[removed]'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forhire[df_forhire['selftext']=='[deleted]'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Out of 1000 posts, 303 were removed by moderators, and 2 were deleted by users themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(df_forhire[df_forhire['selftext'] == '[removed]'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems the forhire subreddit has more posts that moderators may consider spam or inappropriate than the jobs subreddit, or the forhire subreddit has stricter community guidelines \n",
    "- Similar to the jobs subreddit, I will delete all rows where the self text has been removed by mods or deleted by the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forhire = remove_del(df_forhire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "df_forhire.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forhire['text'] = df_forhire['title'] + df_forhire['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df_forhire[df_forhire['text'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df_forhire.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Combine dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forhire.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I have 219 more posts from r/jobs than r/forhire, since many of the posts in r/forhire were removed by moderators\n",
    "- I want balanced classes when I perform my EDA and train my models, so I will take 500 posts from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.sample(n=500, replace = False, random_state = 42).reset_index(drop=True)\n",
    "df_forhire = df_forhire.sample(n=500, replace = False, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined dataframe\n",
    "df = pd.concat([df_jobs,df_forhire]).reset_index(drop = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the title and self text columns\n",
    "df.drop(['title','selftext'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLP Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before I move to my EDA there are some pre-processing steps for text that may be useful\n",
    "- This will help me to find insights from the text data more easily during my Exploratory Data Analysis\n",
    "- These steps are:\n",
    "> 1. Remove line breaks and URLs\n",
    "> 2. Tokenize\n",
    "> 3. Remove stop words\n",
    "> 4. Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Remove line breaks and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert everything to lower string first\n",
    "df['text_adj'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets view a long post\n",
    "df[df['text_adj'].str.len()>500]['text_adj'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to remove line breaks\n",
    "df['text_adj'] = df['text_adj'].map(lambda x: re.sub('\\n', ' ', x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm line breaks removed\n",
    "df['text_adj'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to remove contractions\n",
    " \n",
    "def expand_text(text):\n",
    "    expanded_words = []\n",
    "    for word in text.split():\n",
    "        expanded_words.append(contractions.fix(word))\n",
    "    return ' '.join(expanded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_adj'] = df['text_adj'].apply(lambda x: expand_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm contractions have been removed\n",
    "df['text_adj'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for URLs\n",
    "df[df['text_adj'].str.contains('http')]['text_adj'][27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to remove URLs\n",
    "df['text_adj'] = df['text_adj'].map(lambda x: re.sub('(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-;]*[\\w@?^=%&\\/~+#-])?', ' ', x)) \n",
    "\n",
    "#use regex to remove HTML coding (&amp, &gt, etc)\n",
    "df['text_adj'] = df['text_adj'].map(lambda x: re.sub('-?&\\w+', ' ', x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_adj'][27]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize for words, currencies, or percentages\n",
    "tokenizer = RegexpTokenizer('((?:[A-Za-z]\\.)+|[$¥£€%]{0,1}(?:\\d{1,10}[,. ])*\\d{1,10}[$¥£€%]{0,1}|\\w+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token'] = df['text_adj'].map(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tokens for first row are correct\n",
    "df['token'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token'] = df['token'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token'] = df['token'].apply(lambda x: [lem.lemmatize(i) for i in x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle final dataframe\n",
    "df.to_pickle('datasets/final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Post Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create length and word count features\n",
    "df['text_length'] = df['text'].map(len)\n",
    "df['word_count'] = df['text'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Longest and Shortest posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 shortest posts in jobs\n",
    "df[df['subreddit']=='jobs'].sort_values(by='word_count')[['text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 longest posts in jobs\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 400):\n",
    "  display(df[df['subreddit']=='jobs'].sort_values(by='word_count', ascending=False)[['text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 shortest posts in forhire\n",
    "df[df['subreddit']=='forhire'].sort_values(by='word_count', ascending=True)[['text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 longest posts in forhire\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 400):\n",
    "  display(df[df['subreddit']=='forhire'].sort_values(by='word_count', ascending=False)[['text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Distribution of Post Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text length and word count\n",
    "\n",
    "title_fig = plt.figure(figsize=(16,8))\n",
    "sns.set_palette('bright')\n",
    "\n",
    "# text legnth\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.set_xlabel('Text Length (Characters)', fontsize=12)\n",
    "sns.histplot(data=df[df['text_length']<2500], x=\"text_length\", hue=\"subreddit\", kde = True)\n",
    "\n",
    "# word count\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "sns.histplot(data=df[df['word_count']<500], x=\"word_count\", hue=\"subreddit\", kde = True)\n",
    "ax2.set_xlabel('Word Count', fontsize=12)\n",
    "\n",
    "plt.suptitle('Distribution of Text Length and Word Count for r/jobs and r/forhire', fontsize=15, fontweight = 'bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The text length and word count distributions are similar\n",
    "- The text length and word count distributions for both jobs and forhire are skewed to the right, however, the skew is greater for jobs lengths\n",
    "- Posts in the jobs subreddit tend to be shorter and use fewer words than posts in the forhire subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "# text length\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "sns.barplot(data = df, x = 'text_length', y = 'subreddit')\n",
    "ax1.set_ylabel('subreddit')\n",
    "ax1.set_xlabel('Text Length')\n",
    "\n",
    "# word count\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "sns.barplot(data = df, x = 'word_count', y = 'subreddit')\n",
    "ax2.set_ylabel('')\n",
    "ax2.set_xlabel('Word Count')\n",
    "\n",
    "plt.suptitle('Mean Text Length and Word Count for r/jobs and r/forhire', fontsize=15, fontweight = 'bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On average, posts in the jobs subreddit have as many words as the forhire subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Frequency of Key Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_tokens = df['token'].explode().value_counts()[:20].index\n",
    "top_20_jobs = []\n",
    "top_20_forhire = []\n",
    "for token in top_20_tokens:\n",
    "    top_20_jobs.append(df[df['subreddit']=='jobs']['token'].explode().value_counts()[token])\n",
    "    top_20_forhire.append(df[df['subreddit']=='forhire']['token'].explode().value_counts()[token])\n",
    "    \n",
    "top20 = pd.DataFrame({'jobs': top_20_jobs, 'forhire': top_20_forhire}, index = top_20_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20.plot(kind = 'barh', figsize=(16,8))\n",
    "plt.title('Top 20 Frequent Words', fontsize=15, fontweight = 'bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the 20 most common words (excluding stop words) across both subreddits, it seems that the words are distinctively from either r/jobs or r/forhire, with the exception of 'year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(22,12))\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "jobs_tokens = df[df['subreddit']=='jobs']['token'].explode()\n",
    "jobs_tokens.value_counts()[:20].plot(kind='barh', color = 'orange')\n",
    "ax1.set_title('r/jobs')\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "forhire_tokens = df[df['subreddit']=='forhire']['token'].explode()\n",
    "forhire_tokens.value_counts()[:20].plot(kind='barh')\n",
    "ax2.set_title('r/forhire')\n",
    "\n",
    "plt.suptitle('Top 20 Frequent Words in r/jobs and r/forhire', fontsize=15, fontweight = 'bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words unique in jobs\n",
    "set(jobs_tokens.value_counts()[:20].index) - set(forhire_tokens.value_counts()[:20].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words unique in forhire\n",
    "set(forhire_tokens.value_counts()[:20].index) - set(jobs_tokens.value_counts()[:20].index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in both jobs and forhire\n",
    "set(jobs_tokens.value_counts()[:20].index).intersection(set(forhire_tokens.value_counts()[:20].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As expected, 'job' is the most common word in r/jobs and 'hire' is one of the most common word in r/forhire\n",
    "- I would expect the following words will be important in determining if a post is similar to the content posted in r/jobs:\n",
    ">'job',\n",
    ">'interview',\n",
    ">'position',\n",
    "\n",
    "- Although 'work' is common in r/jobs, it is also common in r/forhire and therefore may not be very helpful when building my classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bigrams are word pairs that help in sentiment analysis\n",
    "- CountVectorizer is used to generate the most frequent bigrams in the full corpus, and both r/jobs and r/forhire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All bigrams from full corpus\n",
    "bigrams_cv = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "bigrams = bigrams_cv.fit_transform(df['text_adj'])\n",
    "bigrams = pd.DataFrame(bigrams.todense(), columns=bigrams_cv.get_feature_names()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams from r/jobs\n",
    "jobsbigrams_cv = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "jobsbigrams = jobsbigrams_cv.fit_transform(df[df['subreddit']=='jobs']['text_adj'])\n",
    "jobsbigrams = pd.DataFrame(jobsbigrams.todense(), columns = jobsbigrams_cv.get_feature_names()).sum()\n",
    "top20jobsbigrams = jobsbigrams.sort_values(ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams from r/forhire\n",
    "forhirebigrams_cv = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "forhirebigrams = forhirebigrams_cv.fit_transform(df[df['subreddit']=='forhire']['text_adj'])\n",
    "forhirebigrams = pd.DataFrame(forhirebigrams.todense(), columns = forhirebigrams_cv.get_feature_names()).sum()\n",
    "top20forhirebigrams = forhirebigrams.sort_values(ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20bigrams = bigrams.sort_values(ascending=False).head(20).index\n",
    "top20_jobs = []\n",
    "top20_forhire = []\n",
    "for bigram in top20bigrams:\n",
    "    try:\n",
    "        top20_jobs.append(jobsbigrams[bigram])\n",
    "    except:\n",
    "        top20_jobs.append(0)\n",
    "    try:\n",
    "        top20_forhire.append(forhirebigrams[bigram])\n",
    "    except:\n",
    "        top20_forhire.append(0)\n",
    "    \n",
    "df_top20bigrams = pd.DataFrame({'jobs': top20_jobs, 'forhire': top20_forhire}, index = top20bigrams)\n",
    "\n",
    "#plot top 20 bigrams\n",
    "df_top20bigrams.plot(kind = 'barh', figsize=(16,8))\n",
    "plt.title('Top 20 Frequent Bigrams in Full Corpus', fontsize=15, fontweight = 'bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the top 20 most frequent bigrams (excluding stop words) such as 'hourly rate', 'content writing', 'graphic designer', 'social media' and 'years experience' are from r/forhire, which makes sense because jobs seekers/jobs postings tend to use similar words to highlight the job requirements or person's capabilities\n",
    "- The most frequent bigrams for r/jobs are 'job offer', 'current job', and 'new job' which makes sense as well since the reddit thread is centered around job advice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(22,12))\n",
    "\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "top20jobsbigrams.plot(kind='barh', color = 'orange')\n",
    "ax1.set_title('r/jobs')\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "top20forhirebigrams.plot(kind='barh')\n",
    "ax2.set_title('r/forhire')\n",
    "\n",
    "plt.suptitle('Top 20 Frequent Bigrams in r/jobs and r/forhire', fontsize=15, fontweight = 'bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams unique in jobs\n",
    "set(top20jobsbigrams.index) - set(top20forhirebigrams.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams unique in forhire\n",
    "set(top20forhirebigrams.index) - set(top20jobsbigrams.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams in both r/jobs and r/forhire\n",
    "set(top20jobsbigrams.index).intersection(set(top20forhirebigrams.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bigrams may be more useful when building my classification model as there is less overlap. The only bigram that is relevant in the top 20 most frequent bigrams for both r/jobs and r/forhire is \"years experience\"\n",
    "- Many of the bigrams I would expect to be unique to one subreddit over the other (such as \"job offer\" or \"graphic designer\" for r/jobs and r/forhire respectively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Other Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['questions'] = df['text_adj'].str.count('\\\\?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('subreddit').sum()['questions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More posts in r/jobs use question marks, which could indicate more people ask questions in r/jobs than in r/forhire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['links'] = df['text'].str.count('http')\n",
    "df.groupby('subreddit').sum()['links']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More posts in r/jobs contain \"http\", which could indicate more people in r/jobs share links than in r/forhire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sentiment Analysis with Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity_scores_neg'] = df['text_adj'].apply(lambda x: sent.polarity_scores(x)['neg'])\n",
    "df['polarity_scores_pos'] = df['text_adj'].apply(lambda x: sent.polarity_scores(x)['pos'])\n",
    "df['polarity_scores_neu'] = df['text_adj'].apply(lambda x: sent.polarity_scores(x)['neu'])\n",
    "df['polarity_scores_comp'] = df['text_adj'].apply(lambda x: sent.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_col = ['subreddit','polarity_scores_neg', 'polarity_scores_pos', 'polarity_scores_neu', 'polarity_scores_comp']\n",
    "df[polarity_col].groupby('subreddit').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(14,10))\n",
    "polar_col = ['polarity_scores_neg', 'polarity_scores_pos', 'polarity_scores_neu', 'polarity_scores_comp']\n",
    "\n",
    "for i,t in enumerate(polar_col):\n",
    "    sns.boxplot(y=t, x=\"subreddit\", data=df, ax=axes[i%2, int(i<2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mean scores for neutral, negative, and positive are similar however the compounded polarity score for r/forhire is significantly higher\n",
    "- There are a lot more outliers present in the positive polarity and negative polarity boxplots for r/jobs, indicating the polarity scores are more extreme\n",
    "- I will look at the 10 posts with the highest negative polarity and positive polarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 negative posts\n",
    "df.sort_values(by='polarity_scores_neg', ascending = False)[['subreddit', 'text_adj', 'polarity_scores_neg']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 9 out of 10 of the highest negative polarity scores are from r/jobs\n",
    "- There seems to be bad experiences around the work environment (colleagues, work scope) which makes sense as posters would seek advice on how to improve their work situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='polarity_scores_pos', ascending = False)[['subreddit', 'text_adj', 'polarity_scores_pos']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The highest positive polarity scores are split between r/jobs and r/forhire\n",
    "- Hiring posts in r/forhire tend to be positive to make the job posting desirable and discussion on job opportunities availale in r/jobs would be optimistic and hence would have have elements of positivity as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reddit's Upvote Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upvote ratio is a measure of how people react to posts in the subreddits and is a float between 0 to 1 (where 1 would mean the post has mostly upvotes compared to downvotes and 0 being the post has the most downvotes). The upvote ratio would show what posts other users in the subreddits agree or disagree with. \n",
    "\n",
    "As I have already cleaned the dataset of posts that were removed by moderators/deleted by users themselves (which would be more likely to have a lower upvote ratio), this feature may not be very useful in understanding how posts are received on reddit and hence I have not considered analysing this feature due to the reason stated and would be dropping this feature in my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map forhire = 1 and jobs = 0\n",
    "\n",
    "df['label'] = df['subreddit'].map({'forhire': 1, 'jobs': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the goal of this project is to be able to predict whether a post belongs to r/forhire or not, I will map r/forhire as True and r/jobs as False\n",
    "\n",
    "This means that:\n",
    " * True Positives are posts that my model correctly predicts are from r/forhire\n",
    " * True Negatives are posts that my model correctly predicts are from r/jobs\n",
    " * False Positves are posts that my model incorrectly predicts are from r/forhire (but are actually from r/jobs)\n",
    " * False Negatives are posts that my model incorrectly predicts are from r/jobs (but are actually from r/forhire)\n",
    "\n",
    "As the purpose of this project is to predict whether a post belongs to r/forhire, therefore, I want to focus on minimizing my False Negative rate, because I want to ensure I target all posts that may be related to r/forhire.\n",
    "\n",
    "At the same time, I do not want to target every post across reddit because this would not be feasible from a time perspective. Hence, I will use the F1 score to ensure my true positives and true negatives are relatively low.\n",
    "\n",
    "I wil train models including a baseline model, Random Forest Classifier, Multinomial Naive Bayes, and Support Vector Classifier to select my best performing model.\n",
    "\n",
    "I will use the following metrics to build and evaluate my models:\n",
    "- F1 score: I will compare the F1 scores of my train, test, and cross validation sets to see if my model is overfitting or underfitting\n",
    "- Recall: I will tune my hyperparameters using recall to minimize my False Negative Rate\n",
    "- ROC AUC, Recall, and F1 score: I will use these three metrics to compare the performance of my models on the test set so that I can select a model that performs well on all three criterias and deliver a final model with an F1 score and sensitivity above 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns except text_adj and label\n",
    "df.drop(['subreddit', 'text', 'token', 'upvote_ratio'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = df['text_adj']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for modeling\n",
    "\n",
    "# compare F1 scores\n",
    "def display_f1(model, X_train, y_train, X_test, y_test):\n",
    "    print('Train F1 Score: ', round(f1_score(y_train, model.predict(X_train)),5))\n",
    "    print('Test F1 Score: ', round(f1_score(y_test, model.predict(X_test)),5))\n",
    "    print('Cross Val F1 Score:', round(cross_val_score(model, X_test,y_test, scoring = 'f1').mean(),5))\n",
    "    \n",
    "# plot ROC and Confusionn matrix\n",
    "def plot_model(model, X_test, y_test):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,7))\n",
    "    \n",
    "    #Plot ROC curve\n",
    "    ax1.set_title('ROC Curve')\n",
    "    plot_roc_curve(model, X_test, y_test, ax = ax1)\n",
    "    ax1.plot([0, 1], [0, 1],label='baseline', linestyle='--')\n",
    "    ax1.legend()\n",
    "\n",
    "    #Plot confusion matrix\n",
    "    ax2.set_title('Confusion Matrix')\n",
    "    y_labeled = y_test.map({1:'forhire', 0:'jobs'})\n",
    "    y_pred = pd.Series(model.predict(X_test)).map({1:'forhire', 0:'jobs'})\n",
    "    cm = confusion_matrix(y_labeled, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax2, cmap='Blues')\n",
    "    ax2.set_xlabel('Predicted labels')\n",
    "    ax2.set_ylabel('True labels')\n",
    "    ax2.xaxis.set_ticklabels(['forhire', 'jobs']) \n",
    "    ax2.yaxis.set_ticklabels(['forhire', 'jobs'])\n",
    "    plt.show();\n",
    "\n",
    "# model comparisons\n",
    "def add_model(name, model, X_test, y_test):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, model.predict(X_test)).ravel()\n",
    "    \n",
    "    model_dictionary[name] = [round(f1_score(y_test, model.predict(X_test)),5), #F1 score\n",
    "                              round(recall_score(y_test, model.predict(X_test)),5), #Recall \n",
    "                              round(roc_auc_score(y_test, model.predict_proba(X_test)[:,1]),5), #ROC AUC \n",
    "                              tp, #True Positive\n",
    "                              fp, #False Positive\n",
    "                              tn, #True Negative\n",
    "                              fn #False Negative\n",
    "                             ]\n",
    "    return pd.DataFrame.from_dict(model_dictionary, orient = 'index', columns=['F1 Score', 'Recall', 'ROC AUC', 'True Positives', 'False Positives','True Negatives','False Negatives'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My baseline model accuracy is 50%. This means that if I were to predict all posts in my test dataset belong to r/forhire, I would have a 50% chance of being correct. \n",
    "- In order to calculate the other metrics I will use to compare my models with my baseline, I will generate baseline predictions for my test dataset by predicting 1 for every instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create baseline dataframe\n",
    "df_baseline = pd.DataFrame(y_test.values, columns=['y_true'])\n",
    "df_baseline['y_pred'] = 1\n",
    "df_baseline['y_pred_prob'] = 0.5\n",
    "\n",
    "df_baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model dictionary to compare future models to baseline\n",
    "model_dictionary = {'Baseline':\n",
    "                    [round(f1_score(df_baseline['y_true'],df_baseline['y_pred']),5), #F1 score\n",
    "                     recall_score(df_baseline['y_true'],df_baseline['y_pred']), # recall score\n",
    "                     roc_auc_score(df_baseline['y_true'], df_baseline['y_pred_prob']), #ROC AUC score\n",
    "                     ((df_baseline['y_pred']==1) & (df_baseline['y_true']==1)).sum(), #True Positive\n",
    "                     ((df_baseline['y_pred']==1) & (df_baseline['y_true']==0)).sum(), #False Positive\n",
    "                     ((df_baseline['y_pred']==0) & (df_baseline['y_true']==0)).sum(), #True Negative\n",
    "                     ((df_baseline['y_pred']==0) & (df_baseline['y_true']==1)).sum() ]} #False Negative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Random Forest (Default Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipe for cvec and random forest\n",
    "\n",
    "rf_default_pipe = Pipeline([\n",
    "                ('cvec', CountVectorizer()),\n",
    "                ('rf', RandomForestClassifier(random_state = 42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_default_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_f1(rf_default_pipe,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest with countvectorizer and no hyperparameter tuning performed well on the train set, but worse on the test and cross validation set, which is an indication that the Model is overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(rf_default_pipe, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model('Random Forest (default)', rf_default_pipe, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model performs better than the baseline for F1 score and I will try tuning my random forest hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Random Forest (HyperParameter Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I will perform a gridsearch for the best hyperparameters for my random forest model tuning for the following hyperparameters:\n",
    " * 100, 250, or 500 trees\n",
    " * CountVectorizer or TfidVectorizer\n",
    " * Unigrams or bigrams included\n",
    "- My gridsearch will choose the hyperparameters that give the best recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([('vec', None), \n",
    "                    ('rf', RandomForestClassifier())])\n",
    "\n",
    "rf_param_grid = {'vec': [CountVectorizer(), TfidfVectorizer()], \n",
    "                 'rf__n_estimators':[100, 250, 500], \n",
    "                 'vec__stop_words': [\"english\"], \n",
    "                 'vec__ngram_range': [(1, 1), (1, 2)]}\n",
    "\n",
    "rf_gs = GridSearchCV(rf_pipe, rf_param_grid, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_f1(rf_gs,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest with hyperparameter tuning does not perform better on both the test set & the cross validation set than the default random forest and the model is still overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(rf_gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display best params\n",
    "rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with hyperparamter tuning, the gridsearch has chosen hyperparameters that are actually similar to the default.\n",
    "The difference is that the number of trees is 250 instead of the default 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model('Random Forest (tuned)', rf_gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the F1 score and recall are above 90% but this model has performed worse than the baseline and default random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tokens with the most importance\n",
    "feat_weights = rf_gs.best_estimator_.named_steps['rf'].feature_importances_\n",
    "rf_tokens = rf_gs.best_estimator_.named_steps['vec'].get_feature_names()\n",
    "\n",
    "rf_feat = pd.DataFrame( {'top_words': rf_tokens, 'importance' : feat_weights})\n",
    "rf_feat = rf_feat.set_index('top_words')\n",
    "rf_feat = rf_feat.sort_values('importance',ascending = False)[:15]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.barh(rf_feat.index, rf_feat['importance'])\n",
    "plt.xlabel('importance')\n",
    "plt.ylabel('token')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the top 20 tokens, there are words I would expect would be good predictors of r/forhire and r/jobs such as \"portfolio\", \"interview\", \"hiring\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.4. Multinomial Naive Bayes (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipe for cvec and multinomialNB\n",
    "\n",
    "nb_default_pipe = Pipeline([\n",
    "                ('cvec', CountVectorizer()),\n",
    "                ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_default_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_f1(nb_default_pipe,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model shows similar overfitting using the default parameters compared to both random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(nb_default_pipe, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model('MultinomialNB (default)', nb_default_pipe, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The F1 score for the multinomial naive bayes model is lower than the random forest model with similar number of false postives to the default random forest model.\n",
    "- I will see if the recall can be improved with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.5. Multinomial Naive Bayes (HyperParameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipe = Pipeline([('vec', None), \n",
    "                    ('nb', MultinomialNB())])\n",
    "\n",
    "nb_param_grid = {'vec': [CountVectorizer(), TfidfVectorizer()], \n",
    "                 'vec__stop_words': [None, 'english'], \n",
    "                 'vec__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'vec__max_features': [None, 1500, 3000, 5000]}\n",
    "\n",
    "nb_gs = GridSearchCV(nb_pipe, nb_param_grid, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_f1(nb_gs,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The train, test, and cross validation F1 scores are closer to each other other than previous models\n",
    "- This model is the least overfit from the models I have constructed so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(nb_gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display best params\n",
    "nb_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Like the default model, the tuned multinomialNB model also used countvectorizer\n",
    "- The only difference between the tuned multinomialNB model and the default is that the tuned model only took the top 5000 most frequent words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model('MultinomialNB (tuned)', nb_gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tuned multinomialNB  performing slightly better than the default multinomialNB for recall and F1 score\n",
    "- Both the tuned multinomialNB and tuned random forest have F1 scores and recall scores that are similar, however, the random forest has a slightly better F1 score and the multinomialNB has a slightly better recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tokens with highest coefficients\n",
    "nb_coef = nb_gs.best_estimator_.named_steps['nb'].coef_[0]\n",
    "nb_tokens = nb_gs.best_estimator_.named_steps['vec'].get_feature_names()\n",
    "\n",
    "nb_feat = pd.DataFrame( {'top_words': nb_tokens, 'importance' : nb_coef})\n",
    "\n",
    "nb_feat = nb_feat.set_index('top_words')\n",
    "nb_feat = nb_feat.sort_values('importance',ascending = False)[:15]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.barh(nb_feat.index, nb_feat['importance'])\n",
    "plt.xlabel('importance')\n",
    "plt.ylabel('token')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since this model did not use stop words, it is interesting that many stop words seem to have high importance in predicting which subreddit a post belongs to with no obvious tokens to predict r/forhire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = Pipeline([('vec', None), \n",
    "                    ('svc', SVC(probability = True))])\n",
    "svc_param_grid = {'vec': [CountVectorizer(), TfidfVectorizer()], \n",
    "                 'vec__stop_words': ['english'], \n",
    "                 'vec__ngram_range': [(1, 1), (1, 2)],\n",
    "                 'svc__kernel': ['linear','rbf', 'sigmoid']}\n",
    "\n",
    "svc_gs = GridSearchCV(svc_pipe, svc_param_grid, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_f1(svc_gs,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The F1 scores are lower for train, test, and cross validation compared to the multinomialNB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(svc_gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display best parameters\n",
    "svc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model('SVC', svc_gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The SVC model has an F1 score and recall above 90%\n",
    "- The SVC actually has the highest recall score from all the models, while still mainting an F1 score above 90%\n",
    "- I will see if I can boost the three best performing models (in terms of recall and F1 scores) using a voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=[('svc', svc_gs), \n",
    "                                    ('rf', rf_gs), \n",
    "                                    ('mnb', nb_gs)], \n",
    "                        voting='soft')\n",
    "vc = vc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_f1(vc,X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This model has the highest F1 score from all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(vc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_model('Voting Classifier', vc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The voting classifier has the highest F1 score, recall, and ROC AUC from all the models\n",
    "- The false negatives in this model is 4 from a test set of 125\n",
    "- The F1 score is 96% and the recall is 97%, which is the higher than the random forest model => therefore I am comfortable using this model to predict whether or not a post belongs to the r/forhire subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to build a binary classification model that could be used to predict if a post on reddit belongs to r/jobs or r/forhire. \n",
    "\n",
    "The final model is a voting classifier that uses a random forest classifier, multinomial naive Bayes, and support vector classifier to predict the subreddit based on the text in a given post. This model has an F1 score of 96% and a recall score of 97%, and would therefore be considered successful.\n",
    "\n",
    "However, these scores are similar to those of the default Random Forest model with countvectorizer and no hyperparameter with both models having a high tendency to overfit the train data from the train_test_split. Though we have selected the model with the least drop in score when testing it on test data, the drop is still rather significant. Thus, the hyperparameter optimisation which was time-consuming only achieved very modest gains in the scores.\n",
    "\n",
    "One reason for this could be that the moderators of the r/forhire do review posts that do not meet the specifics set in the Rules & Guidelines for r/forhire which would result in posts that are not related to job postings being removed regularly and thus resulting in the r/forhire posts being specific to job postings only.\n",
    "\n",
    "<img src='datasets/rforhireguidelinesCapture.jpg' width = 700 align = center>\n",
    "<center><font size=2 color='grey'>(Fig 3. r/forhire Rules and Guidelines as of 9pm, 22 Sep 2021.)</font></center>\n",
    "\n",
    "Thus, the work done by the moderators might have resulted in the default Random Forest model being able to predict accurately if a post on reddit belongs to r/jobs or r/forhire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Recommendations and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations to further improve the model include:\n",
    "    \n",
    "- Expand our model to other subreddits that concern employment such as r/careeradvice to increase the corpus of words\n",
    "- Get more data from other employment resources (e.g. LinkedIn, JobStreet etc.)\n",
    "\n",
    "This is to build up the training data with more word pairs that can be used to distiguish posts better and reinforce the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
